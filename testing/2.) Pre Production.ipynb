{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the markdown blocks that say interaction required! The notebook should take care of the rest!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (data_frame_types.py, line 1454)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/ericcacciavillani/anaconda3/envs/eflow/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3326\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \u001b[1;32m\"<ipython-input-1-c892dd4cf817>\"\u001b[0m, line \u001b[1;32m4\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    from eflow.foundation import DataPipeline,DataFrameTypes\n",
      "\u001b[0;36m  File \u001b[0;32m\"../eflow/foundation/__init__.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from .data_frame_types import DataFrameTypes\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"../eflow/foundation/data_frame_types.py\"\u001b[0;36m, line \u001b[0;32m1454\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "from eflow.foundation import DataPipeline,DataFrameTypes\n",
    "from eflow.data_analysis import FeatureAnalysis, NullAnalysis\n",
    "from eflow.model_analysis import ClassificationAnalysis\n",
    "from eflow.data_pipeline_segments import FeatureTransformer, TypeFixer, DataEncoder\n",
    "from eflow.utils.modeling_utils import optimize_model_grid\n",
    "from eflow.utils.eflow_utils import get_type_holder_from_pipeline, remove_unconnected_pipeline_segments\n",
    "from eflow.utils.math_utils import get_unbalanced_threshold\n",
    "from eflow.utils.sys_utils import create_dir_structure\n",
    "from eflow.utils.eflow_utils import create_color_dict_for_features\n",
    "from eflow.utils.pandas_utils import data_types_table, value_counts_table, suggest_removal_features \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scikitplot as skplt\n",
    "from nltk.corpus import words\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "from IPython.display import clear_output\n",
    "from IPython.core.getipython import get_ipython\n",
    "import ipython_blocking\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional add ons\n",
    "# !pip install pandasgui\n",
    "# !pip install pivottablejs\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juypter notebook generating cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note: Replace if set to True will remove all the contents of whatever cell it is called in. But it can be undone with a simple CMD + Z. ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: http://tinyurl.com/y6mghyzl\n",
    "def create_new_cell(contents,\n",
    "                    replace=False):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates a new jupyter cell.\n",
    "    \"\"\"\n",
    "    shell = get_ipython()\n",
    "    shell.set_next_input(contents,\n",
    "                         replace=replace)\n",
    "\n",
    "def __format_list_to_string(list_name,\n",
    "                            list_contents):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Converts a list to a string and adds newlines for formating.\n",
    "    \"\"\"\n",
    "    output_str = f\"{list_name} = [\"\n",
    "    escape_seq_count = 0\n",
    "    final_index = len(list_contents) - 1\n",
    "    req_spacing = len(output_str)\n",
    "\n",
    "    for i,element in enumerate(list_contents):\n",
    "        if i == final_index:\n",
    "            output_str += f'\\\"{element}\\\"'\n",
    "        else:\n",
    "            output_str += f'\\\"{element}\\\",'\n",
    "        \n",
    "        if len(output_str.split(\"\\n\")[escape_seq_count]) > 78:\n",
    "            output_str += \"\\n\"\n",
    "            output_str += (\" \" * req_spacing)\n",
    "            escape_seq_count += 1\n",
    "    output_str += \"]\"\n",
    "    return output_str\n",
    "\n",
    "def create_new_cell_with_removal_features(df,\n",
    "                                          replace=True):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates a new cell block with a list of suggested features to remove.\n",
    "    \n",
    "    Args:\n",
    "        df:\n",
    "            Pandas DataFrame object\n",
    "            \n",
    "        replace:\n",
    "            Boolean to determine replacing the current cell.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get suggestions for removal\n",
    "    cell_content = __format_list_to_string(\"removal_features\",\n",
    "                                           suggest_removal_features(df))\n",
    "    # Add a sort of calling card of the function that created it\n",
    "    cell_content = f\"# create_new_cell_with_removal_features(df,replace={replace})\\n\" + cell_content\n",
    "    create_new_cell(cell_content,\n",
    "                    replace=replace)\n",
    "\n",
    "def create_new_cell_with_null_removal_features(df,\n",
    "                                               null_threshold=.25,\n",
    "                                               replace=True):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates a new cell block with a list of suggested features to remove based on nulls.\n",
    "    \n",
    "    Args:\n",
    "        df:\n",
    "            Pandas DataFrame object\n",
    "            \n",
    "        null_threshold:\n",
    "            Any features that contain x% percent of nulls are suggested.\n",
    "            \n",
    "        replace:\n",
    "            Boolean to determine replacing the current cell.\n",
    "    \"\"\"\n",
    "    \n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = df.isnull().sum() / len(df)\n",
    "\n",
    "    cell_content = __format_list_to_string(\"remove_null_features\",\n",
    "                                            mis_val_percent[mis_val_percent > null_threshold].index.to_list())\n",
    "    # Add a calling card of the function that created it\n",
    "    cell_content += f\"# create_new_cell_with_null_removal_features(df,null_threshold={null_threshold},replace={replace})\\n\"\n",
    "    create_new_cell(cell_content,\n",
    "                    replace=replace)\n",
    "\n",
    "def create_new_cell_with_feature_value_color_dict(df,\n",
    "                                                  df_features,\n",
    "                                                  value_limit=50,\n",
    "                                                  replace=True):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates a new cell block with a dict of suggested feature value colors.\n",
    "    \n",
    "    Args:\n",
    "        df:\n",
    "            Pandas DataFrame object\n",
    "        \n",
    "        df_features:\n",
    "            DataFrameTypes object.\n",
    "        \n",
    "        null_threshold:\n",
    "            Any features that contain x% percent of nulls are suggested.\n",
    "            \n",
    "        value_limit:\n",
    "            Limit the amount of feature_values until the system will ignore\n",
    "            the feature all together for dict generation.\n",
    "            \n",
    "        replace:\n",
    "            Boolean to determine replacing the current cell.\n",
    "    \"\"\"\n",
    "    feature_value_color_dict = create_color_dict_for_features(df,\n",
    "                                                              df_features,\n",
    "                                                              value_limit)\n",
    "    # Add a sort of calling card of the function that created it\n",
    "    cell_content = \"\"\n",
    "    cell_content += f\"# create_new_cell_with_feature_value_color_dict(df,df_features,value_limit={value_limit},replace={replace})\\n\"\n",
    "    cell_content += \"feature_value_color_dict=dict()\"\n",
    "    feature_count = 0\n",
    "    for feature_name, feature_value_color in feature_value_color_dict.items():\n",
    "        if feature_value_color_dict[feature_name].keys(): \n",
    "            cell_content += f\"\\nfeature_value_color_dict[\\\"{feature_name}\\\"] = dict()\"\n",
    "        else:\n",
    "            cell_content += f\"\\n\\n# The feature '{feature_name}' has to many values! Asserting assumption that you don't want to give colors to each!\"\n",
    "        \n",
    "        for feature_value, color in feature_value_color.items():\n",
    "\n",
    "            color = feature_value_color_dict[feature_name][feature_value]\n",
    "            \n",
    "            if isinstance(feature_value,str):\n",
    "                feature_value = f\"\\\"{feature_value}\\\"\"\n",
    "            else:\n",
    "                feature_value = f\"{feature_value}\"\n",
    "            \n",
    "            if color is None:\n",
    "                cell_content += f\"\\nfeature_value_color_dict[\\\"{feature_name}\\\"][{feature_value}] = None\"\n",
    "            else:\n",
    "                cell_content += f\"\\nfeature_value_color_dict[\\\"{feature_name}\\\"][{feature_value}] = \\\"{color}\\\"\"\n",
    "        cell_content += \"\\n\"\n",
    "        \n",
    "    create_new_cell(cell_content,\n",
    "                    replace=replace)\n",
    "\n",
    "def create_new_cell_with_categorical_dict(df,\n",
    "                                          df_features,\n",
    "                                          value_limit=50,\n",
    "                                          replace=True):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates a new cell block with a dict of\n",
    "    \n",
    "    Args:\n",
    "        df:\n",
    "            Pandas DataFrame object\n",
    "\n",
    "        df_features:\n",
    "            DataFrameTypes object.\n",
    "\n",
    "        value_limit:\n",
    "            Limit the amount of feature_values until the system will ignore\n",
    "            the feature all together for dict generation.\n",
    "            \n",
    "        replace:\n",
    "            Boolean to determine replacing the current cell.\n",
    "    \"\"\"\n",
    "\n",
    "    cell_content = \"\"\n",
    "    cell_content += f\"# create_new_cell_with_categorical_dict(df,df_features,value_limit={value_limit},replace={replace})\\n\"\n",
    "    cell_content += \"categorical_value_dict = dict()\"\n",
    "    for feature_name in df_features.get_categorical_features():\n",
    "        \n",
    "        # Find and sort feature values\n",
    "        feature_values = df[feature_name].value_counts(sort=False).index.to_list()\n",
    "        feature_values = [str(val) for val in feature_values]\n",
    "        feature_values.sort()\n",
    "        \n",
    "        # Create feature cat dict\n",
    "        cat_found = False\n",
    "        categorical_value_dict[feature_name] = dict()\n",
    "        for val in feature_values:\n",
    "            try:\n",
    "                categorical_value_dict[feature_name][int(val)] = \"\"\n",
    "                cat_found = True\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        # Delete feature name if no categories are found\n",
    "        if not cat_found:\n",
    "            del categorical_value_dict[feature_name]\n",
    "    \n",
    "    for feature_name,cat_val_dict in categorical_value_dict.items():\n",
    "        \n",
    "        if len(cat_val_dict.keys()) < value_limit:\n",
    "            cell_content += f\"categorical_value_dict[\\\"{feature_name}\\\"]=dict()\\n\"\n",
    "            for cat,val in cat_val_dict.items():\n",
    "\n",
    "                if isinstance(val,str):\n",
    "                    cell_content += f\"categorical_value_dict[\\\"{feature_name}\\\"][{cat}] = \\\"{val}\\\"\\n\"\n",
    "                else:\n",
    "                    cell_content += f\"categorical_value_dict[\\\"{feature_name}\\\"][{cat}] = {val}\\n\"\n",
    "        else:\n",
    "            cell_content += f\"\\n\\n# The feature '{feature_name}' has to many values! Asserting assumption that you don't want to give encode to each!\"\n",
    "\n",
    "        \n",
    "\n",
    "    create_new_cell(cell_content,\n",
    "                    replace=replace)\n",
    "    \n",
    "    \n",
    "\n",
    "def create_new_cell_with_value_representation(df,\n",
    "                                              df_features,\n",
    "                                              value_limit=50,\n",
    "                                              replace=True):\n",
    "    \"\"\"\n",
    "    Desc:\n",
    "        Creates a new cell block with a dict of suggested feature value colors.\n",
    "    \n",
    "    Args:\n",
    "        df:\n",
    "            Pandas DataFrame object\n",
    "\n",
    "        df_features:\n",
    "            DataFrameTypes object.\n",
    "\n",
    "        value_limit:\n",
    "            Limit the amount of feature_values until the system will ignore\n",
    "            the feature all together for dict generation.\n",
    "            \n",
    "        replace:\n",
    "            Boolean to determine replacing the current cell.\n",
    "    \"\"\"\n",
    "    feature_value_representation = dict()\n",
    "    for feature_name in df_features.get_string_features():\n",
    "        feature_value_representation[feature_name] = dict()\n",
    "        for val in df[feature_name].dropna().value_counts(sort=False).index.to_list():\n",
    "            if isinstance(val,str):\n",
    "                if len(val) == 0:\n",
    "                    continue\n",
    "                if len(val) <= 3 or val not in words.words():\n",
    "                    feature_value_representation[feature_name][val] = \"\"\n",
    "\n",
    "                if len(feature_value_representation[feature_name].keys()) >= 50:\n",
    "                    break\n",
    "\n",
    "        if not len(feature_value_representation[feature_name].keys()):\n",
    "            del feature_value_representation[feature_name]\n",
    "    cell_content = \"\"\n",
    "    cell_content += f\"# create_new_cell_with_value_representation(df,df_features,value_limit={value_limit},replace={replace})\\n\"\n",
    "    \n",
    "    cell_content += \"feature_value_representation = dict()\\n\"\n",
    "    for feature_name,val_repr_dict in feature_value_representation.items():\n",
    "        \n",
    "        if len(val_repr_dict.keys()) < value_limit:\n",
    "            cell_content += f\"feature_value_representation[\\\"{feature_name}\\\"] = dict()\\n\"\n",
    "            for val,reprs in val_repr_dict.items():\n",
    "\n",
    "                if isinstance(val,str):\n",
    "                    cell_content += f\"feature_value_representation[\\\"{feature_name}\\\"][\\\"{val}\\\"] = \"\n",
    "                else:\n",
    "                    cell_content += f\"feature_value_representation[\\\"{feature_name}\\\"][{val}] = \"\n",
    "                \n",
    "                if isinstance(reprs,str):\n",
    "                    cell_content += f\"\\\"{reprs}\\\"\\n\"\n",
    "                else:\n",
    "                    cell_content += f\"{reprs}\\n\"\n",
    "        else:\n",
    "            cell_content += f\"\\n\\n# The feature '{feature_name}' has to many values! Asserting assumption that you don't want to give representation to to each!\"\n",
    "        \n",
    "        cell_content += \"\\n\"\n",
    "    create_new_cell(cell_content,\n",
    "                    replace=replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Project Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Datasets/titanic_train.csv\"\n",
    "\n",
    "# -----\n",
    "dataset_name = \"Titanic Data\"\n",
    "pipeline_name = \"Titanic Pipeline\"\n",
    "\n",
    "# -----\n",
    "\n",
    "\n",
    "# -----\n",
    "notebook_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean out segment space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_unconnected_pipeline_segments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "shape_df = pd.DataFrame.from_dict({'Rows': [df.shape[0]],\n",
    "                                   'Columns': [df.shape[1]]})\n",
    "display(shape_df)\n",
    "display(df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Sex\"][0] = np.nan\n",
    "# df[\"Survived\"] = df[\"Survived\"].astype(\"object\")\n",
    "# df[\"Age\"] = df[\"Age\"].astype('object')\n",
    "# df[\"Age\"][33] = \"      \"\n",
    "# df[\"Pclass\"][50] = \"2sdf,asdqw\"\n",
    "# df[\"SibSp\"][2] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_types_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b8646c7a38f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_types_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_types_table' is not defined"
     ]
    }
   ],
   "source": [
    "data_types_table(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and init df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: 1\n",
    "# df_features = get_type_holder_from_pipeline(pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option: 2\n",
    "df_features = DataFrameTypes()\n",
    "df_features.init_on_json_file(os.getcwd() + f\"/eflow Data/{dataset_name}/df_features.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.display_features(display_dataframes=True,\n",
    "                             notebook_mode=notebook_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup pipeline structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipe = DataPipeline(pipeline_name,\n",
    "                         df,\n",
    "                         df_features,\n",
    "                         remove_past_contents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Unwanted Columns due to illogical nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Any features that have to many nulls/we can't or shouldn't perform any special logic to determine the closest or actual value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_new_cell_with_null_removal_features(df,null_threshold=0.25,replace=True)\n",
    "remove_null_features = [\"Cabin\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add to main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(remove_null_features):\n",
    "    feature_transformer = FeatureTransformer()\n",
    "    feature_transformer.remove_features(df,\n",
    "                                        df_features,\n",
    "                                        remove_null_features)\n",
    "    main_pipe.add(\"Remove unresolvable null features\",\n",
    "                  feature_transformer)\n",
    "\n",
    "    del feature_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# String Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "target_feature = df_features.get_target_feature()\n",
    "i = 0\n",
    "colors = [\"b\", \"g\", \"y\", \"black\"]\n",
    "plt.close()\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "for target_val in df[target_feature].dropna().value_counts().index:\n",
    "    sns.distplot(df[\"Age\"].dropna()[df[target_feature].dropna() == target_val], color=colors[i])\n",
    "    i += 1\n",
    "plt.show()\n",
    "\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(\n",
    "            df[target_feature],\n",
    "            df[\"Pclass\"],\n",
    "            label=\"Total\",\n",
    "            color=\"b\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.jointplot(df[\"Fare\"],df[\"Age\"], color=colors[i])\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "# Show each distribution with both violins and points\n",
    "sns.violinplot(df[target_feature], df[\"Age\"], inner=\"points\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "sns.pairplot(df[[feature_name for feature_name in df.columns if feature_name not in df_features.get_string_features()]], hue=f\"{target_feature}\", diag_kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\n",
    "\n",
    "# Initialize the FacetGrid object\n",
    "pal = sns.cubehelix_palette(10, rot=-.20, light=.7)\n",
    "g = sns.FacetGrid(df,\n",
    "                  row=target_feature,\n",
    "                  hue=target_feature,\n",
    "                  aspect=15,\n",
    "                  height=.4,\n",
    "                  palette=pal)\n",
    "\n",
    "# Draw the densities in a few steps\n",
    "g.map(sns.kdeplot,\n",
    "      \"Age\",\n",
    "      clip_on=False,\n",
    "      shade=True,\n",
    "      alpha=1,\n",
    "      lw=1.5,\n",
    "      bw=.2)\n",
    "g.map(sns.kdeplot,\n",
    "      \"Age\",\n",
    "      clip_on=False,\n",
    "      color=\"w\",\n",
    "      lw=2,\n",
    "      bw=.2)\n",
    "\n",
    "g.map(plt.axhline,\n",
    "      y=0,\n",
    "      lw=2,\n",
    "      clip_on=False)\n",
    "\n",
    "# Define and use a simple function to label the plot in axes coordinates\n",
    "def label(x, color, label):\n",
    "    ax = plt.gca()\n",
    "    ax.text(0, .2,\n",
    "            label,\n",
    "            fontweight=\"bold\",\n",
    "            color=color,\n",
    "            ha=\"left\",\n",
    "            va=\"center\",\n",
    "            transform=ax.transAxes)\n",
    "\n",
    "\n",
    "g.map(label, \"Age\")\n",
    "\n",
    "# Set the subplots to overlap\n",
    "g.fig.subplots_adjust(hspace=-.25)\n",
    "\n",
    "# Remove axes details that don't play well with overlap\n",
    "g.set_titles(\"\")\n",
    "g.set(yticks=[])\n",
    "g.despine(bottom=True, left=True)\n",
    "g.fig.set_size_inches(10, 10, forward=True)\n",
    "g.fig.suptitle(f'{target_feature} by Age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datawig\n",
    "\n",
    "# df_train, df_test = datawig.utils.random_split(df)\n",
    "\n",
    "# #Initialize a SimpleImputer model\n",
    "# imputer = datawig.SimpleImputer(\n",
    "#     input_columns=['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin','Embarked'], # column(s) containing information about the column we want to impute\n",
    "#     output_column= 'Age', # the column we'd like to impute values for\n",
    "#     output_path = 'imputer_model' # stores model data and metrics\n",
    "#     )\n",
    "\n",
    "# #Fit an imputer model on the train data\n",
    "# imputer.fit(train_df=df, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputed_training=mice(df[df_features.get_numerical_features()].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of feature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_analysis = FeatureAnalysis(project_sub_dir=pre_processing_name)\n",
    "feature_analysis.perform_analysis(df,\n",
    "                                  df_features,\n",
    "                                  dataset_name=dataset_name)\n",
    "del feature_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start test modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "test = copy.deepcopy(df)\n",
    "test = test.dropna(axis='columns')\n",
    "test[\"Sex\"] = df[\"Sex\"] == \"female\"\n",
    "y = test[target_feature].values\n",
    "X = test.drop(columns=[target_feature]).values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.80, random_state=517, stratify=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(set(y_train))\n",
    "set(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best parameters for model\n",
    "param_grid = {\n",
    "    \"max_depth\": list(range(1, 4)),\n",
    "#     \"min_samples_leaf\": list(range(80, 130, 5)),\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"n_splits\": [20, 30]\n",
    "}\n",
    "\n",
    "model, best_params = optimize_model_grid(\n",
    "    model=DecisionTreeClassifier(),\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_micro\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_analysis = ClassificationAnalysis(model=model,\n",
    "                                           model_name=repr(model).split(\"(\")[0],\n",
    "                                           target_feature=target_feature,\n",
    "                                           pred_funcs_dict={\"Probabilities function\":model.predict_proba,\n",
    "                                                            \"Predict function\":model.predict},\n",
    "                                           sample_data=X_train[0],\n",
    "                                           project_sub_dir=f'{post_processing_name}/Classification Analysis',\n",
    "                                           notebook_mode=notebook_mode,\n",
    "                                           df_features=df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_analysis.perform_analysis(X=X_train,\n",
    "                                   y=y_train,\n",
    "                                   dataset_name=\"Train Data\",\n",
    "                                   thresholds_matrix=[[.0,.0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model_analysis.perform_analysis(X=X_test,\n",
    "                                   y=y_test,\n",
    "                                   dataset_name=\"Test Data\",\n",
    "                                   thresholds_matrix=[[.0,.0],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eflow._hidden.utils.meta_data_identity import check_create_metadata_of_dataframe\n",
    "testing_path = \"/Users/ericcacciavillani/Desktop/Coding/Python_Files/Artificial_Intelligence/Data Mining/eFlowMaster/Testing/eflow Data/Pre processing/Missing Data/All Data\"\n",
    "check_create_metadata_of_dataframe(df,\n",
    "                                   testing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(y_test, model.predict(X_test), average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, model.predict(X_test), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
