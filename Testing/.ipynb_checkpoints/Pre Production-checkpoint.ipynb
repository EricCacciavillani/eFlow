{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import ipython_blocking\n",
    "from pivottablejs import pivot_ui\n",
    "import scikitplot as skplt\n",
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from eflow import DataFrameTypes\n",
    "from eflow.analysis import DataAnalysis\n",
    "from eflow.pipeline_segments import DataCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be sure to run the following"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare Worflow Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (This should be the only place you should have to declare anything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Datasets/titanic_train.csv\"\n",
    "target_column = \"Survived\"\n",
    "parent_project_name = \"Pre processing\"\n",
    "prediction_method = \"Classification\"\n",
    "notebook_mode = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 12)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(dataset_path)\n",
    "display(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Survived         int64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MissingDataAnalysis' object has no attribute 'data_types_table'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e818c1d22c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m missing_analysis.perform_analysis(df,\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"All Data\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                   null_features_only=True)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Coding/Python_Files/Artificial_Intelligence/Data Mining/eFlowMaster/eflow/analysis/missing_data_analysis.py\u001b[0m in \u001b[0;36mperform_analysis\u001b[0;34m(self, df, dataset_name, null_features_only, display_visuals, save_file)\u001b[0m\n\u001b[1;32m    157\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__called_from_peform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m                 self.data_types_table(df,\n\u001b[0m\u001b[1;32m    160\u001b[0m                                       \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                                       \u001b[0mdisplay_visuals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_visuals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MissingDataAnalysis' object has no attribute 'data_types_table'"
     ]
    }
   ],
   "source": [
    "from eflow.analysis import MissingDataAnalysis\n",
    "missing_analysis = MissingDataAnalysis(sub_dir=parent_project_name,\n",
    "                                       notebook_mode=notebook_mode)\n",
    "missing_analysis.perform_analysis(df,\n",
    "                                  dataset_name=\"All Data\",\n",
    "                                  null_features_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction tool for dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_ui(df,\n",
    "         outfile_path='Piviot_Table_JS.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w = widgets.SelectMultiple(\n",
    "#     options=['Apples', 'Oranges', 'Pears'],\n",
    "#     value=['Oranges'],\n",
    "#     #rows=10,\n",
    "#     description='Fruits',\n",
    "#     disabled=False\n",
    "# )\n",
    "# del w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = str(u\"\\u2192\")\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Un-Wanted Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do not remove nans yet, let the datacleaner do it's job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"Name\",\n",
    "                 \"Ticket\",\n",
    "                 \"PassengerId\"],\n",
    "        inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "dt = parser.parse(\"Aug 28 1999 12:00AM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date_test\"] = [\"2019-01-02\" for _ in range(0,df.shape[0])]\n",
    "df[\"Date_test\"][0] = np.nan\n",
    "# df[\"Date_test\"] = [parser.parse(val)for val in df[\"Date_test\"].value_counts().keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change cabin column to have the level on the ship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Cabin\"] = df[\"Cabin\"].str.replace(r'\\d+', '').str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Feature Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make given data type changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"Pclass\"] = df[\"Pclass\"].replace(1, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final look at data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up DataFrameTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = DataFrameTypes(df,\n",
    "                             target_column=target_column,\n",
    "                             ignore_nulls=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skim through Value Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col not in df_features.get_float_features() and len(np.unique(df[col].dropna().values)) <= 12:\n",
    "        display(df[col].value_counts())\n",
    "        print(\"***\" * 4 + \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform quick analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_obj = DataAnalysis(df,\n",
    "                            df_features,\n",
    "                            project_name=f'{parent_project_name}/General Analysis (Before Cleaning)',\n",
    "                            missing_data_visuals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner = DataCleaner(df,\n",
    "                           project_name=f'{parent_project_name}/Data Cleaning',\n",
    "                           missing_data_visuals=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner.data_cleaning_widget(df,\n",
    "                                  df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner.get_last_saved_json_file_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaner.data_cleaning_with_json_file(df,\n",
    "                                          data_cleaner.get_last_saved_json_file_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from impyute.imputation.cs import mice\n",
    "\n",
    "a = df[\"Age\"].tolist()\n",
    "# start the MICE training\n",
    "imputed_training=mice(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datawig\n",
    "\n",
    "df_train, df_test = datawig.utils.random_split(df)\n",
    "\n",
    "#Initialize a SimpleImputer model\n",
    "imputer = datawig.SimpleImputer(\n",
    "    input_columns=['Survived', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin','Embarked'], # column(s) containing information about the column we want to impute\n",
    "    output_column= 'Age', # the column we'd like to impute values for\n",
    "    output_path = 'imputer_model' # stores model data and metrics\n",
    "    )\n",
    "\n",
    "#Fit an imputer model on the train data\n",
    "imputer.fit(train_df=df, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_training=mice(df[df_features.get_numerical_features()].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datawig\n",
    "# !pip install opencv-python\n",
    "# !pip install Pillow\n",
    "# !pip install tesserocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "df[\"Cabin\"] = df[\"Cabin\"].fillna(\"A\")\n",
    "test = df.drop(columns=[\"Date_test\", \"Embarked\"]).dropna()\n",
    "\n",
    "test[\"Cabin\"] = df[\"Cabin\"] == \"B\"\n",
    "test[\"Sex\"] = df[\"Sex\"] == \"male\"\n",
    "test[target_column] = [random.randint(0, 10) for _ in range(0,test.shape[0])]\n",
    "print(len(test[target_column]))\n",
    "\n",
    "y = test[target_column].values\n",
    "X = test.values\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "df_features = DataFrameTypes(test,\n",
    "                             target_column=target_column,\n",
    "                             ignore_nulls=True) \n",
    "df_features.get_all_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=517, stratify=y,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from eflow.modeling import optimize_model_grid\n",
    "# Find best parameters for model\n",
    "param_grid = {\n",
    "    \"max_depth\": list(range(2, 3)),\n",
    "    \"min_samples_leaf\": list(range(80, 130, 5)),\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"n_splits\": [20, 30]\n",
    "}\n",
    "\n",
    "model, best_params = optimize_model_grid(\n",
    "    model=DecisionTreeClassifier(),\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    param_grid=param_grid\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eflow._hidden.Objects.enum import enum\n",
    "from eflow.utils.sys_utils import create_plt_png, convert_to_filename, \\\n",
    "    df_to_image, write_object_text_to_file, get_unique_directory_path, \\\n",
    "    pickle_object_to_file\n",
    "from eflow._hidden.Objects.FileOutput import *\n",
    "from eflow._hidden.CustomExc import *\n",
    "from eflow.analysis import DataAnalysis\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "import scikitplot as skplt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import copy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ClassificationAnalysis(FileOutput):\n",
    "\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 model_name,\n",
    "                 pred_funcs_dict,\n",
    "                 sample_data,\n",
    "                 project_name=\"Classification analysis_objects\",\n",
    "                 overwrite_full_path=None,\n",
    "                 notebook_mode=True,\n",
    "                 target_classes=None,\n",
    "                 df_features=None,\n",
    "                 columns=[],\n",
    "                 save_model=True):\n",
    "        \"\"\"\n",
    "        model:\n",
    "            A fitted supervised machine learning model.\n",
    "\n",
    "        model_name:\n",
    "            The name of the model in string form.\n",
    "\n",
    "        pred_funcs_dict:\n",
    "            A dict of the name of the function and the function defintion for the\n",
    "            model prediction methods.\n",
    "            (Can handle either a return of probabilities or a singile value.)\n",
    "            Init Example:\n",
    "            pred_funcs = dict()\n",
    "            pred_funcs[\"Predictions\"] = model.predict\n",
    "            pred_funcs[\"Probabilities\"] = model.probas\n",
    "         \n",
    "        sample_data:\n",
    "            Given data to then pass into our prediction functions to get a\n",
    "            resultant to get the classification prediction 'type'. \n",
    "\n",
    "        project_name:\n",
    "            Creates a parent or \"project\" folder in which all sub-directories\n",
    "            will be inner nested.\n",
    "\n",
    "        overwrite_full_path:\n",
    "            Overwrites the path to the parent folder.\n",
    "\n",
    "        notebook_mode:\n",
    "            If in a python notebook display in the notebook.\n",
    "\n",
    "        target_classes:\n",
    "            Specfied list/np.array of targeted classes the model predicts. If set to\n",
    "            none then it will attempt to pull from the sklearn's default attribute\n",
    "            '.classes_'.\n",
    "\n",
    "        df_features:\n",
    "            DataFrameTypeHolder object. If initalized we can run correct/error\n",
    "            analysis on the dataframe. Will save object in a pickle file and provided columns\n",
    "            if initalized and df_features is not initalized.\n",
    "\n",
    "        columns:\n",
    "            Will overwrite over df_features (DataFrameTypeHolder) regardless \n",
    "            of whether or not df_features is init.\n",
    "            \n",
    "        Returns/Desc:\n",
    "            Evaluates the given model based on the prediction functions pased to it.\n",
    "            Saves the model and other various graphs/dataframes for evaluation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Init any parent objects\n",
    "        FileOutput.__init__(self,\n",
    "                            f'{project_name}/{model_name}',\n",
    "                            overwrite_full_path)\n",
    "\n",
    "        # Init objects without pass by refrence\n",
    "        self.__model = copy.deepcopy(model)\n",
    "        self.__model_name = copy.deepcopy(model_name)\n",
    "        self.__notebook_mode = copy.deepcopy(notebook_mode)\n",
    "        self.__target_values = copy.deepcopy(target_classes)\n",
    "        self.__df_features = copy.deepcopy(df_features)\n",
    "        self.__pred_funcs_dict = copy.deepcopy(pred_funcs_dict)\n",
    "        self.__pred_funcs_types = dict()\n",
    "\n",
    "        # Init on sklearns default target classes attribute\n",
    "        if not self.__target_values:\n",
    "            self.__target_values = copy.deepcopy(model.classes_)\n",
    "        # ---\n",
    "        if len(self.__target_values) != 2:\n",
    "            self.__binary_classifcation = False\n",
    "        else:\n",
    "            self.__binary_classifcation = True\n",
    "\n",
    "        # Save machine learning model\n",
    "        if save_model:\n",
    "            pickle_object_to_file(self.__model,\n",
    "                                  self.get_output_folder(),\n",
    "                                  f'{self.__model_name}')\n",
    "\n",
    "        # ---\n",
    "        check_create_dir_structure(self.get_output_folder(),\n",
    "                                   \"Extras\")\n",
    "        # Save predicted classes\n",
    "        write_object_text_to_file(self.__target_values,\n",
    "                                  self.get_output_folder() + \"Extras\",\n",
    "                                  \"_Classes\")\n",
    "\n",
    "        # Save features and or df_features object\n",
    "        if columns or df_features:\n",
    "            if columns:\n",
    "                write_object_text_to_file(columns,\n",
    "                                          self.get_output_folder() + \"Extras\",\n",
    "                                          \"_Features\")\n",
    "            else:\n",
    "                write_object_text_to_file(df_features.get_all_features(),\n",
    "                                          self.get_output_folder() + \"Extras\",\n",
    "                                          \"_Features\")\n",
    "                pickle_object_to_file(self.__model,\n",
    "                                      self.get_output_folder() + \"Extras\",\n",
    "                                      \"_df_features\")\n",
    "\n",
    "        # Find the 'type' of each prediction. Probabilities or Predictions\n",
    "        if self.__pred_funcs_dict:\n",
    "            for pred_name, pred_func in self.__pred_funcs_dict.items():\n",
    "                model_output = pred_func(\n",
    "                    np.reshape(sample_data[0],\n",
    "                               (-1, sample_data.shape[1])))[0]\n",
    "\n",
    "                # Confidence / Probability (Continuous output)\n",
    "                if isinstance(model_output, list) or isinstance(model_output,\n",
    "                                                                np.ndarray):\n",
    "                    self.__pred_funcs_types[pred_name] = \"Probabilities\"\n",
    "\n",
    "                    # Classification (Discrete output)\n",
    "                else:\n",
    "                    self.__pred_funcs_types[pred_name] = \"Predictions\"\n",
    "        else:\n",
    "            raise RequiresPredictionMethods\n",
    "\n",
    "    def __get_model_prediction(self,\n",
    "                               pred_name,\n",
    "                               X,\n",
    "                               thresholds=None):\n",
    "        \"\"\"\n",
    "        X:\n",
    "            Feature matrix.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        Returns/Desc:\n",
    "            Returns back a predicted value based for a given matrix.\n",
    "            Handles prediction function 'types' Predictions and Probabilities.\n",
    "            Helps streamline the entire process of evaluating classes.\n",
    "        \"\"\"\n",
    "        # DEBUG_MARKER\n",
    "\n",
    "        # Must be a prediction function\n",
    "        if self.__pred_funcs_types[pred_name] == \"Predictions\":\n",
    "            return self.__pred_funcs_dict[pred_name](X)\n",
    "\n",
    "        elif self.__pred_funcs_types[pred_name] == \"Probabilities\":\n",
    "            \n",
    "            # Validate probabilities\n",
    "            if thresholds:\n",
    "                if isinstance(thresholds, list) or \\\n",
    "                        isinstance(thresholds, np.ndarray):\n",
    "                    if len(thresholds) != len(self.__target_values):\n",
    "                        raise ThresholdLength\n",
    "                else:\n",
    "                    raise ThresholdType\n",
    "\n",
    "            model_output = self.__get_model_probas(pred_name,\n",
    "                                                   X)\n",
    "            if not thresholds:\n",
    "                return np.asarray([self.__target_values[np.argmax(proba)]\n",
    "                                   for proba in model_output])\n",
    "                \n",
    "            \n",
    "            bool_matrix_thresholds = model_output < thresholds\n",
    "            \n",
    "            prob_passed_matrix = np.asarray([\n",
    "                np.asarray([model_output[i][0] if passed else float(\"-inf\") \n",
    "                            for i,passed in enumerate(bool_vector)])\n",
    "                for bool_vector in bool_matrix_thresholds])            \n",
    "\n",
    "            model_predictions = np.asarray([self.__target_values[np.argmax(proba_vector)]\n",
    "                                            if sum(proba_vector != float(\"-inf\")) > 0 \n",
    "                                            else self.__target_values[np.argmax(model_output)]\n",
    "                                            for proba_vector in prob_passed_matrix])\n",
    "            return model_predictions\n",
    "        else:\n",
    "            raise UnknownModelOutputType\n",
    "\n",
    "    def __get_model_probas(self,\n",
    "                           pred_name,\n",
    "                           X):\n",
    "        \"\"\"\n",
    "        X:\n",
    "            Feature matrix.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        Returns/Desc:\n",
    "            Returns back a series of values between 0-1 to represent it's confidence.\n",
    "            Invokes an error if the prediction function call is anything but a Probabilities\n",
    "            call.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.__pred_funcs_types[pred_name] == \"Probabilities\":\n",
    "            model_output = self.__pred_funcs_dict[pred_name](X)\n",
    "\n",
    "            # ---\n",
    "            if isinstance(model_output, list):\n",
    "                model_output = np.asarray(model_output)\n",
    "\n",
    "            return model_output\n",
    "        else:\n",
    "            raise ProbasNotPossible\n",
    "\n",
    "    def __create_sub_dir_with_thresholds(self,\n",
    "                                         pred_name,\n",
    "                                         dataset_name,\n",
    "                                         thresholds):\n",
    "        \"\"\"\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The passed in dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        Returns/Desc:\n",
    "            Looking at the root of the starting directory and looking at each\n",
    "            '_Thresholds.txt' file to determine if the files can be outputed\n",
    "            to that directory. The content of the file must match the content\n",
    "            of the list/numpy array 'thresholds'.\n",
    "        \"\"\"\n",
    "        sub_dir = f'{dataset_name}/{pred_name}'\n",
    "\n",
    "        # Only generate extra folder structure if function type is Probabilities\n",
    "        if self.__pred_funcs_types[pred_name] == \"Probabilities\":\n",
    "\n",
    "            # ------\n",
    "            if not thresholds:\n",
    "                sub_dir = f'{sub_dir}/No Thresholds'\n",
    "            else:\n",
    "                i = 0\n",
    "                sub_dir = f'{sub_dir}/Thresholds'\n",
    "                tmp_sub_dir = copy.deepcopy(sub_dir)\n",
    "                while True:\n",
    "                    threshold_dir = self.get_output_folder()\n",
    "                    if i > 0:\n",
    "                        tmp_sub_dir = (sub_dir + f' {i}')\n",
    "                    threshold_dir += tmp_sub_dir\n",
    "\n",
    "                    # If file exists with the same thresholds; than use this directory\n",
    "                    if os.path.exists(threshold_dir):\n",
    "                        if self.__compare_thresholds_to_saved_thresholds(\n",
    "                                threshold_dir,\n",
    "                                thresholds):\n",
    "                            sub_dir = tmp_sub_dir\n",
    "                            break\n",
    "\n",
    "                    # Create new directory\n",
    "                    else:\n",
    "                        os.makedirs(threshold_dir)\n",
    "                        write_object_text_to_file(thresholds,\n",
    "                                                  threshold_dir,\n",
    "                                                  \"_Thresholds\")\n",
    "                        sub_dir = tmp_sub_dir\n",
    "                        break\n",
    "\n",
    "                    # Iterate for directory name change\n",
    "                    i += 1\n",
    "\n",
    "        return sub_dir\n",
    "\n",
    "    def __compare_thresholds_to_saved_thresholds(self,\n",
    "                                                 directory_pth,\n",
    "                                                 thresholds):\n",
    "        \"\"\"\n",
    "        directory_pth:\n",
    "            Path to the given folder where the \"_Thresholds.txt\"\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        Returns/Desc:\n",
    "            Compare the thresholds object to the text file; returns true if\n",
    "            the file exists and the object's value matches up.\n",
    "        \"\"\"\n",
    "\n",
    "        file_directory = correct_directory_path(directory_pth)\n",
    "\n",
    "        if os.path.exists(file_directory):\n",
    "\n",
    "            # Extract file contents and convert to a list object\n",
    "            file = open(file_directory + \"_Thresholds.txt\", \"r\")\n",
    "            line = file.read()\n",
    "            converted_list = line.split(\"=\")[-1].strip().strip('][').split(\n",
    "                ', ')\n",
    "            converted_list = [float(val) for val in converted_list]\n",
    "            file.close()\n",
    "\n",
    "            if thresholds == converted_list:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def peform_analysis(self,\n",
    "                        X,\n",
    "                        y,\n",
    "                        dataset_name,\n",
    "                        thresholds_matrix=None,\n",
    "                        figsize=(10, 8),\n",
    "                        normalize_confusion_matrix=True,\n",
    "                        ignore_metrics=[],\n",
    "                        custom_metrics=dict(),\n",
    "                        average_scoring=[\"micro\",\n",
    "                                         \"macro\",\n",
    "                                         \"weighted\"],\n",
    "                        display_analysis_graphs=False):\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds_matrix:\n",
    "            List of list/Matrix of thresholds\n",
    "\n",
    "            each thresholds:\n",
    "                If the model outputs a probability list/numpy array then we apply\n",
    "                thresholds to the ouput of the model.\n",
    "                For classification only; will not affect the direct output of\n",
    "                the probabilities.\n",
    "\n",
    "        figsize:\n",
    "            Plot's dimension's.\n",
    "\n",
    "        normalize_confusion_matrix:\n",
    "            Normalize the confusion matrix buckets.\n",
    "\n",
    "        ignore_metrics:\n",
    "            Specify set metrics to ignore. (F1-Score, Accuracy etc).\n",
    "\n",
    "        ignore_metrics:\n",
    "            Specify the default metrics to not apply to the classification\n",
    "            analysis.\n",
    "                * Precision\n",
    "                * MCC\n",
    "                * Recall\n",
    "                * F1-Score\n",
    "                * Accuracy\n",
    "\n",
    "        custom_metrics:\n",
    "            Pass the name of metric(s) and the function definition(s) in a\n",
    "            dictionary.\n",
    "\n",
    "        average_scoring:\n",
    "            Determines the type of averaging performed on the data.\n",
    "\n",
    "        display_analysis_graphs:\n",
    "            Controls visual display of error error analysis if it is able to run.\n",
    "\n",
    "        Returns/Desc:\n",
    "            Performs all classification functionality with the provided feature\n",
    "            data and target data.\n",
    "                * plot_precision_recall_curve\n",
    "                * classification_evaluation\n",
    "                * plot_confusion_matrix\n",
    "        \"\"\"\n",
    "        if isinstance(thresholds_matrix, np.ndarray):\n",
    "            thresholds_matrix = thresholds_matrix.tolist()\n",
    "\n",
    "        if not thresholds_matrix:\n",
    "            thresholds_matrix = list()\n",
    "\n",
    "        if isinstance(thresholds_matrix, list) and not isinstance(\n",
    "                thresholds_matrix[0], list):\n",
    "            thresholds_matrix = list(thresholds_matrix)\n",
    "        \n",
    "        \n",
    "        if None not in thresholds_matrix:\n",
    "            thresholds_matrix.append(None)\n",
    "\n",
    "        print(\"\\n\\n\" + \"---\" * 10 + f'{dataset_name}' + \"---\" * 10)\n",
    "\n",
    "        for pred_name, pred_type in self.__pred_funcs_types.items():\n",
    "            print(f\"Now running classification on {pred_name}\", end = '')\n",
    "            print(\"hit\")\n",
    "            for thresholds in thresholds_matrix:\n",
    "                if pred_type == \"Predictions\":\n",
    "                    thresholds = None\n",
    "                else:\n",
    "                    if thresholds:\n",
    "                        print(f\"on thresholds:\\n{thresholds}\")\n",
    "                    else:\n",
    "                        print(\"No thresholds\")\n",
    "                \n",
    "                self.classification_metrics(X,\n",
    "                                            y,\n",
    "                                            pred_name=pred_name,\n",
    "                                            dataset_name=dataset_name,\n",
    "                                            thresholds=thresholds,\n",
    "                                            ignore_metrics=ignore_metrics,\n",
    "                                            custom_metrics=custom_metrics,\n",
    "                                            average_scoring=average_scoring)\n",
    "\n",
    "                self.plot_confusion_matrix(X,\n",
    "                                           y,\n",
    "                                           pred_name=pred_name,\n",
    "                                           dataset_name=dataset_name,\n",
    "                                           thresholds=thresholds,\n",
    "                                           figsize=figsize,\n",
    "                                           normalize=normalize_confusion_matrix)\n",
    "\n",
    "                if pred_type == \"Probabilities\":\n",
    "                    self.plot_precision_recall_curve(X,\n",
    "                                                     y,\n",
    "                                                     pred_name=pred_name,\n",
    "                                                     dataset_name=dataset_name,\n",
    "                                                     figsize=figsize,\n",
    "                                                     thresholds=thresholds)\n",
    "                    self.plot_roc_curve(X,\n",
    "                                        y,\n",
    "                                        pred_name=pred_name,\n",
    "                                        dataset_name=dataset_name,\n",
    "                                        figsize=figsize,\n",
    "                                        thresholds=thresholds)\n",
    "\n",
    "                    if self.__binary_classifcation:\n",
    "                        self.plot_lift_curve(X,\n",
    "                                             y,\n",
    "                                             pred_name=pred_name,\n",
    "                                             dataset_name=dataset_name,\n",
    "                                             figsize=figsize,\n",
    "                                             thresholds=thresholds)\n",
    "                        self.plot_ks_statistic(X,\n",
    "                                               y,\n",
    "                                               pred_name=pred_name,\n",
    "                                               dataset_name=dataset_name,\n",
    "                                               figsize=figsize,\n",
    "                                               thresholds=thresholds)\n",
    "                        self.plot_calibration_curve(X,\n",
    "                                                    y,\n",
    "                                                    pred_name=pred_name,\n",
    "                                                    dataset_name=dataset_name,\n",
    "                                                    figsize=figsize,\n",
    "                                                    thresholds=thresholds)\n",
    "                        self.plot_cumulative_gain(X,\n",
    "                                                  y,\n",
    "                                                  pred_name=pred_name,\n",
    "                                                  dataset_name=dataset_name,\n",
    "                                                  figsize=figsize,\n",
    "                                                  thresholds=thresholds)\n",
    "                        \n",
    "                if self.__df_features:\n",
    "                    self.classification_error_analysis(X,\n",
    "                                                       y,\n",
    "                                                       pred_name=pred_name,\n",
    "                                                       dataset_name=dataset_name,\n",
    "                                                       thresholds=thresholds,\n",
    "                                                       display_analysis_graphs=display_analysis_graphs)\n",
    "\n",
    "                    if pred_type == \"Predictions\":\n",
    "                        break\n",
    "\n",
    "    def plot_calibration_curve(self,\n",
    "                               X,\n",
    "                               y,\n",
    "                               pred_name,\n",
    "                               dataset_name,\n",
    "                               thresholds=None,\n",
    "                               save_file=True,\n",
    "                               title=None,\n",
    "                               ax=None,\n",
    "                               cmap='nipy_spectral',\n",
    "                               figsize=None,\n",
    "                               title_fontsize='large',\n",
    "                               text_fontsize='medium'):\n",
    "\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "        \n",
    "        save_file:\n",
    "            Boolean value to wether or not to save the file. \n",
    "\n",
    "        From scikit-plot documentation (Note not all attributes are provided to you):\n",
    "        Link: http://tinyurl.com/y3ym5pyc\n",
    "        Returns/Descr:\n",
    "            Plots calibration curves for a set of classifier probability estimates.\n",
    "        \"\"\"\n",
    "        \n",
    "        filename = f'KS Statistic on {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "        if not title:\n",
    "            title = filename\n",
    "\n",
    "        skplt.metrics.plot_calibration_curve(y,\n",
    "                                             self.__get_model_probas(pred_name,\n",
    "                                                                     X),\n",
    "                                             title=title,\n",
    "                                             ax=ax,\n",
    "                                             cmap=cmap,\n",
    "                                             figsize=figsize,\n",
    "                                             title_fontsize=title_fontsize,\n",
    "                                             text_fontsize=text_fontsize)\n",
    "\n",
    "        if save_file:\n",
    "            create_plt_png(self.get_output_folder(),\n",
    "                           sub_dir,\n",
    "                           convert_to_filename(filename))\n",
    "\n",
    "        if self.__notebook_mode:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def plot_roc_curve(self,\n",
    "                       X,\n",
    "                       y,\n",
    "                       pred_name,\n",
    "                       dataset_name,\n",
    "                       thresholds=None,\n",
    "                       save_file=True,\n",
    "                       title=None,\n",
    "                       ax=None,\n",
    "                       figsize=(10, 8),\n",
    "                       title_fontsize='large',\n",
    "                       text_fontsize='medium'):\n",
    "\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "        \n",
    "        save_file:\n",
    "            Boolean value to wether or not to save the file. \n",
    "\n",
    "        From scikit-plot documentation (Note not all attributes are provided to you):\n",
    "        Link: http://tinyurl.com/y3ym5pyc\n",
    "        Returns/Descr:\n",
    "            Creates ROC curves from labels and predicted probabilities.\n",
    "        \"\"\"\n",
    "        filename = f'Roc Curve on {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "        if not title:\n",
    "            title = filename\n",
    "\n",
    "        skplt.metrics.plot_roc(y,\n",
    "                               self.__get_model_probas(pred_name,\n",
    "                                                       X),\n",
    "                               title=title,\n",
    "                               ax=ax,\n",
    "                               figsize=figsize,\n",
    "                               title_fontsize=title_fontsize,\n",
    "                               text_fontsize=text_fontsize)\n",
    "\n",
    "        if save_file:\n",
    "            create_plt_png(self.get_output_folder(),\n",
    "                           sub_dir,\n",
    "                           convert_to_filename(filename))\n",
    "\n",
    "        if self.__notebook_mode:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def plot_cumulative_gain(self,\n",
    "                             X,\n",
    "                             y,\n",
    "                             pred_name,\n",
    "                             dataset_name,\n",
    "                             thresholds=None,\n",
    "                             save_file=True,\n",
    "                             title=None,\n",
    "                             ax=None,\n",
    "                             figsize=(10, 8),\n",
    "                             title_fontsize='large',\n",
    "                             text_fontsize='medium'):\n",
    "\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'.\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        save_file:\n",
    "            Boolean value to wether or not to save the file. \n",
    "\n",
    "        From scikit-plot documentation (Note not all attributes are provided to you):\n",
    "        Link: http://tinyurl.com/y3ym5pyc\n",
    "        Returns/Descr:\n",
    "        \"\"\"\n",
    "        filename = f'Cumulative Gain gain on {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "        if not title:\n",
    "            title = filename\n",
    "\n",
    "        skplt.metrics.plot_cumulative_gain(y,\n",
    "                                           self.__get_model_probas(pred_name,\n",
    "                                                                   X),\n",
    "                                           title=title,\n",
    "                                           ax=ax,\n",
    "                                           figsize=figsize,\n",
    "                                           title_fontsize=title_fontsize,\n",
    "                                           text_fontsize=text_fontsize)\n",
    "\n",
    "        if save_file:\n",
    "            create_plt_png(self.get_output_folder(),\n",
    "                           sub_dir,\n",
    "                           convert_to_filename(filename))\n",
    "\n",
    "        if self.__notebook_mode:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def plot_precision_recall_curve(self,\n",
    "                                    X,\n",
    "                                    y,\n",
    "                                    pred_name,\n",
    "                                    dataset_name,\n",
    "                                    thresholds=None,\n",
    "                                    save_file=True,\n",
    "                                    title=None,\n",
    "                                    plot_micro=True,\n",
    "                                    classes_to_plot=None,\n",
    "                                    ax=None,\n",
    "                                    figsize=(10, 8),\n",
    "                                    cmap='nipy_spectral',\n",
    "                                    title_fontsize='large',\n",
    "                                    text_fontsize='medium'):\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        From scikit-plot documentation (Note not all attributes are provided to you):\n",
    "        Link: http://tinyurl.com/y3ym5pyc\n",
    "        Returns/Descr:\n",
    "            Creates a plot precision recall curve plot based on the models predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        filename = f'Precision Recall on {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "        if not title:\n",
    "            title = filename\n",
    "\n",
    "        skplt.metrics.plot_precision_recall(y,\n",
    "                                            self.__get_model_probas(pred_name,\n",
    "                                                                    X),\n",
    "                                            title=title,\n",
    "                                            plot_micro=plot_micro,\n",
    "                                            classes_to_plot=classes_to_plot,\n",
    "                                            ax=ax,\n",
    "                                            figsize=figsize,\n",
    "                                            cmap=cmap,\n",
    "                                            title_fontsize=title_fontsize,\n",
    "                                            text_fontsize=text_fontsize)\n",
    "\n",
    "        if save_file:\n",
    "            create_plt_png(self.get_output_folder(),\n",
    "                           sub_dir,\n",
    "                           convert_to_filename(filename))\n",
    "\n",
    "        if self.__notebook_mode:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def plot_lift_curve(self,\n",
    "                        X,\n",
    "                        y,\n",
    "                        pred_name,\n",
    "                        dataset_name,\n",
    "                        thresholds=None,\n",
    "                        save_file=True,\n",
    "                        title=None,\n",
    "                        ax=None,\n",
    "                        figsize=(10, 8),\n",
    "                        title_fontsize='large',\n",
    "                        text_fontsize='medium'):\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        From scikit-plot documentation (Note not all attributes are provided to you):\n",
    "        Link: http://tinyurl.com/y3ym5pyc\n",
    "        Returns/Descr:\n",
    "            Creates a plot precision recall curve plot based on the models predictions.\n",
    "        \"\"\"\n",
    "\n",
    "        filename = f'Lift Curve on {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "        if not title:\n",
    "            title = filename\n",
    "\n",
    "        skplt.metrics.plot_lift_curve(y,\n",
    "                                      self.__get_model_probas(pred_name,\n",
    "                                                              X),\n",
    "                                      thresholds=thresholds,\n",
    "                                      title=title,\n",
    "                                      ax=ax,\n",
    "                                      figsize=figsize,\n",
    "                                      title_fontsize=title_fontsize,\n",
    "                                      text_fontsize=text_fontsize)\n",
    "        if save_file:\n",
    "            create_plt_png(self.get_output_folder(),\n",
    "                           sub_dir,\n",
    "                           convert_to_filename(filename))\n",
    "\n",
    "        if self.__notebook_mode:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def plot_confusion_matrix(self,\n",
    "                              X,\n",
    "                              y,\n",
    "                              pred_name,\n",
    "                              dataset_name,\n",
    "                              thresholds=None,\n",
    "                              save_file=True,\n",
    "                              title=None,\n",
    "                              normalize=False,\n",
    "                              hide_zeros=False,\n",
    "                              hide_counts=False,\n",
    "                              x_tick_rotation=0,\n",
    "                              ax=None,\n",
    "                              figsize=(10, 8),\n",
    "                              cmap='Blues',\n",
    "                              title_fontsize='large',\n",
    "                              text_fontsize='medium'):\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        From scikit-plot documentation (Note not all attributes are provided to you):\n",
    "        Link: http://tinyurl.com/y3ym5pyc\n",
    "        Returns/Descr:\n",
    "            Creates a confusion matrix plot based on the models predictions.\n",
    "        \"\"\"\n",
    "        filename = f'Confusion Matrix: {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "        if not title:\n",
    "            title = filename\n",
    "\n",
    "        warnings.filterwarnings('ignore')\n",
    "        skplt.metrics.plot_confusion_matrix(\n",
    "            self.__get_model_prediction(pred_name,\n",
    "                                        X,\n",
    "                                        thresholds),\n",
    "            y,\n",
    "            title=title,\n",
    "            normalize=normalize,\n",
    "            hide_zeros=hide_zeros,\n",
    "            hide_counts=hide_counts,\n",
    "            x_tick_rotation=x_tick_rotation,\n",
    "            ax=ax,\n",
    "            figsize=figsize,\n",
    "            cmap=cmap,\n",
    "            title_fontsize=title_fontsize,\n",
    "            text_fontsize=text_fontsize)\n",
    "        warnings.filterwarnings('default')\n",
    "\n",
    "        if save_file:\n",
    "            create_plt_png(self.get_output_folder(),\n",
    "                           sub_dir,\n",
    "                           convert_to_filename(filename))\n",
    "\n",
    "        if self.__notebook_mode:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def classification_metrics(self,\n",
    "                               X,\n",
    "                               y,\n",
    "                               pred_name,\n",
    "                               dataset_name,\n",
    "                               thresholds=None,\n",
    "                               save_file=True,\n",
    "                               title=\"\",\n",
    "                               custom_metrics=dict(),\n",
    "                               ignore_metrics=[],\n",
    "                               average_scoring=[\"micro\",\n",
    "                                                \"macro\",\n",
    "                                                \"weighted\"]):\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned stored\n",
    "            in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        save_file:\n",
    "            Determines whether or not to save the generated document.\n",
    "\n",
    "        title:\n",
    "            Adds to the column 'Metric Score'.\n",
    "\n",
    "        sub_dir:\n",
    "            Specify a subdirectory to append to the output path of the file.\n",
    "\n",
    "        custom_metrics:\n",
    "            Pass the name of metric(s) and the function definition(s) in a\n",
    "            dictionary.\n",
    "\n",
    "        ignore_metrics:\n",
    "            Specify the default metrics to not apply to the classification\n",
    "            analysis.\n",
    "                * Precision\n",
    "                * MCC\n",
    "                * Recall\n",
    "                * F1-Score\n",
    "                * Accuracy\n",
    "\n",
    "        average_scoring:\n",
    "            Determines the type of averaging performed on the data.\n",
    "                * micro\n",
    "                * macro\n",
    "                * weighted\n",
    "\n",
    "        Returns/Desc:\n",
    "            Creates/displays a dataframe object based on the model's\n",
    "            predictions on the feature matrix compared to target data.\n",
    "        \"\"\"\n",
    "        filename = f'Metric Evaluation on {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "\n",
    "        if not isinstance(average_scoring, list):\n",
    "            average_scoring = [average_scoring]\n",
    "\n",
    "        # Default metric name's and their function\n",
    "        metric_functions = dict()\n",
    "        metric_functions[\"Precision\"] = precision_score\n",
    "        metric_functions[\"MCC\"] = matthews_corrcoef\n",
    "        metric_functions[\"Recall\"] = recall_score\n",
    "        metric_functions[\"F1-Score\"] = f1_score\n",
    "        metric_functions[\"Accuracy\"] = accuracy_score\n",
    "\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "        # Ignore default metrics if needed\n",
    "        for remove_metric in ignore_metrics:\n",
    "            if remove_metric in metric_functions:\n",
    "                del metric_functions[remove_metric]\n",
    "\n",
    "        # Add in custom metrics\n",
    "        if len(custom_metrics.keys()):\n",
    "            metric_functions.update(custom_metrics)\n",
    "\n",
    "        # Evaluate model on metrics\n",
    "        evaluation_report = dict()\n",
    "        for metric_name in metric_functions:\n",
    "            for average_score in average_scoring:\n",
    "\n",
    "                model_predictions = self.__get_model_prediction(pred_name,\n",
    "                                                                X,\n",
    "                                                                thresholds)\n",
    "                try:\n",
    "                    evaluation_report[f'{metric_name}({average_score})'] = \\\n",
    "                        metric_functions[metric_name](y_true=y,\n",
    "                                                      y_pred=model_predictions,\n",
    "                                                      average=average_score)\n",
    "                except TypeError:\n",
    "                    evaluation_report[metric_name] = metric_functions[\n",
    "                        metric_name](y,\n",
    "                                     model_predictions)\n",
    "                    break\n",
    "\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "        warnings.filterwarnings('default')\n",
    "\n",
    "        if title and len(title) > 0:\n",
    "            index_name = f\"Metric Scores ({title})\"\n",
    "        else:\n",
    "            index_name = \"Metric Scores\"\n",
    "\n",
    "        # ---\n",
    "        evaluation_report = pd.DataFrame({index_name:\n",
    "                                              [f'{metric_score:.4f}'\n",
    "                                               for metric_score\n",
    "                                               in evaluation_report.values()]},\n",
    "                                         index=list(evaluation_report.keys()))\n",
    "\n",
    "        if self.__notebook_mode:\n",
    "            display(evaluation_report)\n",
    "        else:\n",
    "            print(evaluation_report)\n",
    "\n",
    "        if save_file:\n",
    "            # Create image file\n",
    "            df_to_image(evaluation_report,\n",
    "                        self.get_output_folder(),\n",
    "                        sub_dir,\n",
    "                        convert_to_filename(filename),\n",
    "                        col_width=20,\n",
    "                        show_index=True,\n",
    "                        format_float_pos=4)\n",
    "\n",
    "    def classification_error_analysis(self,\n",
    "                                      X,\n",
    "                                      y,\n",
    "                                      pred_name,\n",
    "                                      dataset_name,\n",
    "                                      thresholds=None,\n",
    "                                      save_file=True,\n",
    "                                      display_analysis_graphs=False):\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        save_file:\n",
    "            Determines whether or not to save the generated document.\n",
    "            \n",
    "        display_analysis_graphs:\n",
    "            Controls visual display of graph generation.\n",
    "\n",
    "        Returns/Descr:\n",
    "            Creates a directory structure of subsetted data produced by all correctly/predicted.\n",
    "        \"\"\"\n",
    "\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "\n",
    "        model_predictions = self.__get_model_prediction(pred_name,\n",
    "                                                        X,\n",
    "                                                        thresholds=thresholds)\n",
    "\n",
    "        if sum(model_predictions == y):\n",
    "            if display_analysis_graphs:\n",
    "                print(\"\\n\\n\" + \"*\" * 10 +\n",
    "                      \"Correctly predicted analysis\"\n",
    "                      + \"*\" * 10 + \"\\n\")\n",
    "            else:\n",
    "                print(\"\\n\\n\" + \"*\" * 10 +\n",
    "                      \"Generating graphs for model's correctly predicted...\" +\n",
    "                      \"*\" * 10 + \"\\n\")\n",
    "                DataAnalysis(pd.DataFrame(X[model_predictions == y],\n",
    "                                          columns=self.__df_features.get_all_features()),\n",
    "                             self.__df_features,\n",
    "                             overwrite_full_path=self.get_output_folder() +\n",
    "                                                 sub_dir + \"/Correctly Predicted Data/\",\n",
    "                             missing_data_visuals=False,\n",
    "                             notebook_mode=display_analysis_graphs)\n",
    "        else:\n",
    "            print(\"Your model predicted nothing correctly...dam that sucks\")\n",
    "\n",
    "        if sum(model_predictions != y):\n",
    "            if display_analysis_graphs:\n",
    "                print(\"\\n\\n\" + \"*\" * 10 +\n",
    "                      \"Incorrectly predicted analysis\"\n",
    "                      + \"*\" * 10 + \"\\n\")\n",
    "            else:\n",
    "                print(\"\\n\\n\" + \"*\" * 10 +\n",
    "                      \"Generating graphs for model's incorrectly predicted...\" +\n",
    "                      \"*\" * 10 + \"\\n\")\n",
    "            \n",
    "#             for target_value in self.__target_values:\n",
    "                \n",
    "\n",
    "            DataAnalysis(pd.DataFrame(X[model_predictions != y],\n",
    "                                      columns=self.__df_features.get_all_features()),\n",
    "                         self.__df_features,\n",
    "                         overwrite_full_path=self.get_output_folder() +\n",
    "                                             sub_dir + \"/Incorrectly Predicted Data/\",\n",
    "                         missing_data_visuals=False,\n",
    "                         notebook_mode=display_analysis_graphs)\n",
    "        else:\n",
    "            print(\n",
    "                \"\\n\\nYour model predicted everything correctly...there is something very wrong here...\")\n",
    "\n",
    "    def classification_report(self,\n",
    "                              X,\n",
    "                              y,\n",
    "                              pred_name,\n",
    "                              dataset_name,\n",
    "                              thresholds=None,\n",
    "                              save_file=True):\n",
    "        \"\"\"\n",
    "        X/y:\n",
    "            Feature matrix/Target data vector.\n",
    "\n",
    "        pred_name:\n",
    "            The name of the prediction function in questioned\n",
    "            stored in 'self.__pred_funcs_dict'\n",
    "\n",
    "        dataset_name:\n",
    "            The dataset's name.\n",
    "\n",
    "        thresholds:\n",
    "            If the model outputs a probability list/numpy array then we apply\n",
    "            thresholds to the ouput of the model.\n",
    "            For classification only; will not affect the direct output of\n",
    "            the probabilities.\n",
    "\n",
    "        save_file:\n",
    "            Determines whether or not to save the generated document.\n",
    "\n",
    "        Returns/Descr:\n",
    "            Creates a report of all target's metric evaluations\n",
    "            based on the model's prediction output.\n",
    "        \"\"\"\n",
    "        filename = f'Classification Report {dataset_name}'\n",
    "        sub_dir = self.__create_sub_dir_with_thresholds(pred_name,\n",
    "                                                        dataset_name,\n",
    "                                                        thresholds)\n",
    "\n",
    "        # Create dataframe report\n",
    "        report_df = pd.DataFrame(classification_report(y,\n",
    "                                                       self.__get_model_prediction(\n",
    "                                                           pred_name,\n",
    "                                                           X,\n",
    "                                                           thresholds),\n",
    "                                                       output_dict=True))\n",
    "\n",
    "        # ---\n",
    "        if self.__notebook_mode:\n",
    "            display(report_df)\n",
    "        else:\n",
    "            print(report_df)\n",
    "\n",
    "        if save_file:\n",
    "            # Output dataframe as png\n",
    "            df_to_image(report_df,\n",
    "                        self.get_output_folder(),\n",
    "                        sub_dir,\n",
    "                        filename,\n",
    "                        col_width=20,\n",
    "                        show_index=True,\n",
    "                        format_float_pos=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_analysis = ClassificationAnalysis(model=model,\n",
    "                                     pred_funcs_dict={\"Probabilities function\":model.predict_proba,\n",
    "                                                      \"Predict function\":model.predict},\n",
    "                                     sample_data=X_train,\n",
    "                                     model_name=repr(model).split(\"(\")[0],\n",
    "                                     project_name=f'{parent_project_name}/Classification Analysis',\n",
    "                                     notebook_mode=True,\n",
    "                                     df_features=df_features)\n",
    "\n",
    "dt_analysis.perform_analysis(X=X_train,\n",
    "                             y=y_train,\n",
    "                             dataset_name=\"Training Data\",\n",
    "                             thresholds_matrix=[[.2,.2,.2,.2,.2,.2,.2,.2,.2,.2,.2],\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.exists(\"/Users/ericcacciavillani/Desktop/Coding/Python_Files/Artificial_Intelligence/Data Mining/eFlowMaster/Testing/eFlow Data/Pre processing/Supervised Analysis/DecisionTreeClassifier/Probabilities function/Thresholds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_unique_directory_path(os.getcwd() + \"/eFlow Data/Pre processing/Supervised Analysis/DecisionTreeClassifier/Test data/Probability Classification/\",\n",
    "                        \"Model Results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_string = os.getcwd().replace(\"/\", \"///\")\n",
    "error_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = \"/Users/ericcacciavillani/Desktop/Coding/Python_Files/Artificial_Intelligence/Data Mining/eFlowMaster/Testing/eFlow Data/Pre processing/Supervised Analysis/DecisionTreeClassifier/Test data/Probability Classification\"\n",
    "correct_directory_path(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[[1,2],[1,2,3],[1]]\n",
    "c = copy.deepcopy(a)\n",
    "b=np.array(a)\n",
    "b.tolist()\n",
    "hhh = None\n",
    "if hhh:\n",
    "    print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb = None\n",
    "\n",
    "if not bbb:\n",
    "    print(\"test\")\n",
    "else:\n",
    "    print(\"fff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_y = copy.deepcopy(y_test)\n",
    "vector_y = np.where(vector_y==0, \"Test\", vector_y) \n",
    "vector_y = np.where(vector_y=='1', \"Blarg\", vector_y)\n",
    "vector_y = np.where(vector_y=='2', \"Dragon\", vector_y)\n",
    "vector_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.metrics.plot_confusion_matrix(vector_y, vector_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0, 0, 8.3, 0, 0, 0, 0, 0, 0, 0, .36]\n",
    "\n",
    "model_output = model.predict_proba(X_train)\n",
    "\n",
    "print(model_output)\n",
    "# Validate probabilities\n",
    "if thresholds:\n",
    "    if isinstance(thresholds, list) or \\\n",
    "            isinstance(thresholds, np.ndarray):\n",
    "        if sum(thresholds) < .98:\n",
    "            print(\"Thresholds didn't add up to 98%-100%! \"\n",
    "                  \"This may cause issues in your results!\")\n",
    "    else:\n",
    "        raise ThresholdType\n",
    "\n",
    "# ---\n",
    "if isinstance(model_output, list):\n",
    "    model_output = np.asarray(model_output)\n",
    "\n",
    "if isinstance(model_output, np.ndarray):\n",
    "    if thresholds:\n",
    "        outputs_passed_threshold = model_output > np.asarray(thresholds)\n",
    "outputs_passed_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "class UnExpectedData(UserWarning, ValueError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model.predict_proba(X_train)\n",
    "bool_matrix_thresholds = model_output < np.asarray([.2,.2,.2,.2,.2,.2,.2,.2,.2,.2,.2])\n",
    "\n",
    "tmp_matrix = []\n",
    "for bool_vector in bool_matrix_thresholds:\n",
    "    tmp_vector = []\n",
    "    for i,passed in enumerate(bool_vector):\n",
    "        if passed:\n",
    "            tmp_vector.append(model_output[i][0])\n",
    "        else:\n",
    "            tmp_vector.append(float(\"-inf\"))\n",
    "    tmp_matrix.append(tmp_vector)\n",
    "print(tmp_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.isnull().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = msno.bar(df[df.columns[df.isna().any()].tolist()],\n",
    "              color=\"#072F5F\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.isnull().sum().index.tolist()\n",
    "null_values = df.isnull().sum().values.tolist()\n",
    "null_sorted_features, null_values = zip(*sorted(zip(null_values,\n",
    "                                                    features)))\n",
    "\n",
    "for feature_index, value in enumerate(null_values):\n",
    "    if value == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cabin': {'Random Value 1': 'NaN', 'Random Value 2': 'NaN', 'Random Value 3': 'NaN', 'Random Value 4': 'NaN', 'Random Value 5': 'NaN', 'Random Value 6': 'NaN', 'Random Value 7': 'NaN', 'Random Value 8': 'NaN', 'Random Value 9': 'B57 B59 B63 B66', 'Random Value 10': 'NaN'}, 'Embarked': {'Random Value 1': 'S', 'Random Value 2': 'S', 'Random Value 3': 'C', 'Random Value 4': 'S', 'Random Value 5': 'S', 'Random Value 6': 'S', 'Random Value 7': 'C', 'Random Value 8': 'C', 'Random Value 9': 'S', 'Random Value 10': 'S'}, 'Name': {'Random Value 1': 'Davies, Mr. Charles Henry', 'Random Value 2': 'Madigan, Miss. Margaret \"Maggie\"', 'Random Value 3': 'Markoff, Mr. Marin', 'Random Value 4': 'Markun, Mr. Johann', 'Random Value 5': 'Herman, Mrs. Samuel (Jane Laver)', 'Random Value 6': 'de Mulder, Mr. Theodore', 'Random Value 7': 'Keane, Miss. Nora A', 'Random Value 8': 'van Billiard, Mr. Austin Blyler', 'Random Value 9': 'Osman, Mrs. Mara', 'Random Value 10': 'Mellors, Mr. William John'}, 'Parch': {'Random Value 1': 'NaN', 'Random Value 2': 'NaN', 'Random Value 3': '1', 'Random Value 4': '1', 'Random Value 5': 'NaN', 'Random Value 6': 'NaN', 'Random Value 7': 'NaN', 'Random Value 8': '2', 'Random Value 9': 'NaN', 'Random Value 10': 'NaN'}, 'PassengerId': {'Random Value 1': '219', 'Random Value 2': '370', 'Random Value 3': '189', 'Random Value 4': '612', 'Random Value 5': '742', 'Random Value 6': '656', 'Random Value 7': '640', 'Random Value 8': '539', 'Random Value 9': '389', 'Random Value 10': '256'}, 'Pclass': {'Random Value 1': '3', 'Random Value 2': '1', 'Random Value 3': '3', 'Random Value 4': '3', 'Random Value 5': '2', 'Random Value 6': '2', 'Random Value 7': '2', 'Random Value 8': '2', 'Random Value 9': '3', 'Random Value 10': '1'}, 'Sex': {'Random Value 1': 'male', 'Random Value 2': 'male', 'Random Value 3': 'female', 'Random Value 4': 'female', 'Random Value 5': 'male', 'Random Value 6': 'female', 'Random Value 7': 'male', 'Random Value 8': 'male', 'Random Value 9': 'female', 'Random Value 10': 'male'}, 'SibSp': {'Random Value 1': 'NaN', 'Random Value 2': '1', 'Random Value 3': '1', 'Random Value 4': 'NaN', 'Random Value 5': '1', 'Random Value 6': 'NaN', 'Random Value 7': 'NaN', 'Random Value 8': 'NaN', 'Random Value 9': 'NaN', 'Random Value 10': 'NaN'}, 'Survived': {'Random Value 1': '1', 'Random Value 2': 'NaN', 'Random Value 3': '1', 'Random Value 4': 'NaN', 'Random Value 5': 'NaN', 'Random Value 6': '1', 'Random Value 7': 'NaN', 'Random Value 8': '1', 'Random Value 9': '1', 'Random Value 10': 'NaN'}, 'Ticket': {'Random Value 1': '367230', 'Random Value 2': 'STON/O 2. 3101275', 'Random Value 3': '110413', 'Random Value 4': 'SO/C 14885', 'Random Value 5': '243847', 'Random Value 6': 'SOTON/O.Q. 3101311', 'Random Value 7': '13502', 'Random Value 8': '230136', 'Random Value 9': '370376', 'Random Value 10': 'SOTON/OQ 392090'}}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "{'Cabin': {'Random Value 1': 'NaN', 'Random Value 2': 'NaN', 'Random Value 3': 'NaN', 'Random Value 4': 'NaN', 'Random Value 5': 'NaN', 'Random Value 6': 'NaN', 'Random Value 7': 'NaN', 'Random Value 8': 'NaN', 'Random Value 9': 'B57 B59 B63 B66', 'Random Value 10': 'NaN'}, 'Embarked': {'Random Value 1': 'S', 'Random Value 2': 'S', 'Random Value 3': 'C', 'Random Value 4': 'S', 'Random Value 5': 'S', 'Random Value 6': 'S', 'Random Value 7': 'C', 'Random Value 8': 'C', 'Random Value 9': 'S', 'Random Value 10': 'S'}, 'Name': {'Random Value 1': 'Davies, Mr. Charles Henry', 'Random Value 2': 'Madigan, Miss. Margaret \"Maggie\"', 'Random Value 3': 'Markoff, Mr. Marin', 'Random Value 4': 'Markun, Mr. Johann', 'Random Value 5': 'Herman, Mrs. Samuel (Jane Laver)', 'Random Value 6': 'de Mulder, Mr. Theodore', 'Random Value 7': 'Keane, Miss. Nora A', 'Random Value 8': 'van Billiard, Mr. Austin Blyler', 'Random Value 9': 'Osman, Mrs. Mara', 'Random Value 10': 'Mellors, Mr. William John'}, 'Parch': {'Random Value 1': 'NaN', 'Random Value 2': 'NaN', 'Random Value 3': '1', 'Random Value 4': '1', 'Random Value 5': 'NaN', 'Random Value 6': 'NaN', 'Random Value 7': 'NaN', 'Random Value 8': '2', 'Random Value 9': 'NaN', 'Random Value 10': 'NaN'}, 'PassengerId': {'Random Value 1': '219', 'Random Value 2': '370', 'Random Value 3': '189', 'Random Value 4': '612', 'Random Value 5': '742', 'Random Value 6': '656', 'Random Value 7': '640', 'Random Value 8': '539', 'Random Value 9': '389', 'Random Value 10': '256'}, 'Pclass': {'Random Value 1': '3', 'Random Value 2': '1', 'Random Value 3': '3', 'Random Value 4': '3', 'Random Value 5': '2', 'Random Value 6': '2', 'Random Value 7': '2', 'Random Value 8': '2', 'Random Value 9': '3', 'Random Value 10': '1'}, 'Sex': {'Random Value 1': 'male', 'Random Value 2': 'male', 'Random Value 3': 'female', 'Random Value 4': 'female', 'Random Value 5': 'male', 'Random Value 6': 'female', 'Random Value 7': 'male', 'Random Value 8': 'male', 'Random Value 9': 'female', 'Random Value 10': 'male'}, 'SibSp': {'Random Value 1': 'NaN', 'Random Value 2': '1', 'Random Value 3': '1', 'Random Value 4': 'NaN', 'Random Value 5': '1', 'Random Value 6': 'NaN', 'Random Value 7': 'NaN', 'Random Value 8': 'NaN', 'Random Value 9': 'NaN', 'Random Value 10': 'NaN'}, 'Survived': {'Random Value 1': '1', 'Random Value 2': 'NaN', 'Random Value 3': '1', 'Random Value 4': 'NaN', 'Random Value 5': 'NaN', 'Random Value 6': '1', 'Random Value 7': 'NaN', 'Random Value 8': '1', 'Random Value 9': '1', 'Random Value 10': 'NaN'}, 'Ticket': {'Random Value 1': '367230', 'Random Value 2': 'STON/O 2. 3101275', 'Random Value 3': '110413', 'Random Value 4': 'SO/C 14885', 'Random Value 5': '243847', 'Random Value 6': 'SOTON/O.Q. 3101311', 'Random Value 7': '13502', 'Random Value 8': '230136', 'Random Value 9': '370376', 'Random Value 10': 'SOTON/OQ 392090'}}\n"
     ]
    }
   ],
   "source": [
    "from eflow._hidden.utils.meta_data_identity import check_create_metadata_of_dataframe\n",
    "testing_path = \"/Users/ericcacciavillani/Desktop/Coding/Python_Files/Artificial_Intelligence/Data Mining/eFlowMaster/Testing/eflow Data/Pre processing/Missing Data/All Data\"\n",
    "check_create_metadata_of_dataframe(df,\n",
    "                                   testing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "feature_name = \"Feature_name\"\n",
    "for hash_type in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    result = 0\n",
    "    for char_index, char in enumerate(feature_name):\n",
    "        if hash_type == 1:\n",
    "            result += int(ord(char))\n",
    "        elif hash_type == 2:\n",
    "            result += int(ord(char) + 62 * ord(char))\n",
    "        elif hash_type == 3:\n",
    "            result += int(ord(char) + 147 * ord(char))\n",
    "        elif hash_type == 4:\n",
    "            result += int((ord(char) + 92) * math.pow(ord(char), 3))\n",
    "        elif hash_type == 5:\n",
    "            result += int(ord(char) + 49 * math.pow(ord(char), 2))\n",
    "        elif hash_type == 6:\n",
    "            result += int((23 + ord(char) + 45) * (3 + ord(char) + 2))\n",
    "        elif hash_type == 7:\n",
    "            result += int((ord(char) * 5) + 32 + 8)\n",
    "        elif hash_type == 8:\n",
    "            result += int(math.pow(ord(char), 2))\n",
    "        elif hash_type == 9:\n",
    "            result += int(ord(char) * 2 + 32 + ord(char) * 2 + 5)\n",
    "        elif hash_type == 10:\n",
    "            result += int(ord(char) * 12 + 76 + math.pow(ord(char), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ticket\"][891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1,2,3]) % 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Age\" in {'Aged', 'Fare'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
